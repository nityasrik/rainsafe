{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16d7ca6-06f7-4946-95ad-4e495862575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "print(\"✅ Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5201c5a7-6e5b-4fe7-abe6-5b4ca49ec93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded and preprocessed.\n",
      "\n",
      "--- Processed Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Rainfall (mm)            10000 non-null  float64\n",
      " 1   Temperature (°C)         10000 non-null  float64\n",
      " 2   Humidity (%)             10000 non-null  float64\n",
      " 3   River Discharge (m³/s)   10000 non-null  float64\n",
      " 4   Water Level (m)          10000 non-null  float64\n",
      " 5   Elevation (m)            10000 non-null  float64\n",
      " 6   Population Density       10000 non-null  float64\n",
      " 7   Infrastructure           10000 non-null  int64  \n",
      " 8   Historical Floods        10000 non-null  int64  \n",
      " 9   Land Cover_Agricultural  10000 non-null  bool   \n",
      " 10  Land Cover_Desert        10000 non-null  bool   \n",
      " 11  Land Cover_Forest        10000 non-null  bool   \n",
      " 12  Land Cover_Urban         10000 non-null  bool   \n",
      " 13  Land Cover_Water Body    10000 non-null  bool   \n",
      " 14  Soil Type_Clay           10000 non-null  bool   \n",
      " 15  Soil Type_Loam           10000 non-null  bool   \n",
      " 16  Soil Type_Peat           10000 non-null  bool   \n",
      " 17  Soil Type_Sandy          10000 non-null  bool   \n",
      " 18  Soil Type_Silt           10000 non-null  bool   \n",
      "dtypes: bool(10), float64(7), int64(2)\n",
      "memory usage: 800.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Load the historical dataset\n",
    "df = pd.read_csv('../data/flood_risk_dataset_india.csv')\n",
    "\n",
    "# --- Preprocessing ---\n",
    "# 1. One-hot encode the categorical features\n",
    "# Corrected this line to use spaces to match the actual column names\n",
    "df_processed = pd.get_dummies(df, columns=['Land Cover', 'Soil Type'])\n",
    "\n",
    "\n",
    "# --- The rest of your code ---\n",
    "# 2. Define the features (X) and the target (y)\n",
    "# Make sure the target column name also matches the file\n",
    "y = df_processed['Flood Occurred'] \n",
    "# Drop the original columns plus the target and location data\n",
    "X = df_processed.drop(columns=['Flood Occurred', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Ensure all feature columns are numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "print(\"✅ Data loaded and preprocessed.\")\n",
    "print(\"\\n--- Processed Data Info ---\")\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c351f59-95b0-40ff-b19c-5700b38b9681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data split and scaled.\n",
      "Training data shape: (8000, 19)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Feature Scaling ---\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the SAME scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Data split and scaled.\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81377b8-3eb9-4790-bd6d-fa23ce7eb643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "✅ Model training complete.\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.47      0.48       989\n",
      "           1       0.50      0.52      0.51      1011\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.49      0.49      0.49      2000\n",
      "weighted avg       0.49      0.49      0.49      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "rfc_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "rfc_model.fit(X_train_scaled, y_train)\n",
    "print(\"✅ Model training complete.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "y_pred = rfc_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0e5ea7-ee91-4c8d-bcd1-9c6629c5157c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to 'model.pkl'\n",
      "✅ Scaler saved to 'scaler.pkl'\n",
      "✅ Feature list saved to 'model_features.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Artifacts ---\n",
    "# 1. Save the trained model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(rfc_model, file)\n",
    "print(\"✅ Model saved to 'model.pkl'\")\n",
    "\n",
    "# 2. Save the fitted scaler\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "print(\"✅ Scaler saved to 'scaler.pkl'\")\n",
    "\n",
    "# 3. Save the list of feature columns\n",
    "feature_names = X.columns.tolist()\n",
    "with open('model_features.pkl', 'wb') as file:\n",
    "    pickle.dump(feature_names, file)\n",
    "print(\"✅ Feature list saved to 'model_features.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd99999-7656-4004-bef4-99a5994b78d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085b204-9f07-461b-a1bd-8be3a4e522df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
